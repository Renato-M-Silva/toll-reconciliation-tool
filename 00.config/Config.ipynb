{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration for local database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark warehouse to: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n",
      "Changed current working directory to: C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Notebook: 00_config_spark.ipynb \"\"\"\n",
    "\n",
    "# Importing libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Define the catalog name\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the layer names\n",
    "bronze_layer = \"bronze\"\n",
    "silver_layer = \"silver\"\n",
    "gold_layer = \"gold\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\"\"\"\n",
    "\n",
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized with app name: toll_reconciliation_tool\n",
      "Databases 'bronze', 'silver', and 'gold' created (if they didn't exist) in: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the bronze database (if it doesn't exist)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_layer}\")\n",
    "\n",
    "# Create the silver database (if it doesn't exist)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_layer}\")\n",
    "\n",
    "# Create the gold database (if it doesn't exist)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_layer}\")\n",
    "\n",
    "print(f\"Spark session initialized with app name: {spark.sparkContext.appName}\")\n",
    "print(f\"Databases '{bronze_layer}', '{silver_layer}', and '{gold_layer}' created (if they didn't exist) in: {warehouse_location}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metastore URI: default\n",
      "['bronze', 'default', 'gold', 'silver']\n",
      "The 'bronze' schema already exists.\n"
     ]
    }
   ],
   "source": [
    "# Print Metastore URI for debugging\n",
    "print(f\"Metastore URI: {spark.conf.get('hive.metastore.uris', 'default')}\")  # Get with default to avoid error\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "print(schemas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Attempt a basic Hive operation\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can stop the Spark session here\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
