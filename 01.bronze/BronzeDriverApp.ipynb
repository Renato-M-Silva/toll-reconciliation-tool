{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or add data to tvde_earnings_history table in the bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, regexp_extract, when, to_date, unix_timestamp, from_unixtime, concat_ws, lpad, create_map\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark warehouse to: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n",
      "Changed current working directory to: C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "# layer and table\n",
    "layer = \"bronze\"\n",
    "table = \"tvde_earnings_history\"\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Functions (select_files, add_files_to_list, normalize_spark_dataframe) ---\n",
    "def select_files():\n",
    "    \"\"\"Opens a file selection dialog and returns a list with the paths of the selected files.\"\"\"\n",
    "    root = tk.Tk() # Create a Tkinter window\n",
    "    root.withdraw() # Hide the main window\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select files\",  # Window title\n",
    "    )\n",
    "    return list(file_paths) # Convert the returned tuple to a list\n",
    "\n",
    "def add_files_to_list(file_list):\n",
    "    \"\"\"Adds the paths of the selected files to the list.\"\"\"\n",
    "    selected_file_paths = select_files()\n",
    "    if selected_file_paths: # Check if the user selected any files\n",
    "        file_list.extend(selected_file_paths)  # Add the paths to the list\n",
    "        print(\"Files added:\")\n",
    "        for file_path in selected_file_paths:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "\n",
    "def normalize_spark_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a Spark DataFrame according to specified rules, using keyword lists.\"\"\"\n",
    "    \n",
    "    # Define lists of keywords for toll, service fee, and tip\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(\n",
    "        r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Extract the year if found, otherwise set to None (as a Spark literal).\n",
    "    year = lit(year_match.group(0)) if year_match else lit(None)\n",
    "\n",
    "    # Extract the lowercase month string if found, otherwise set to None (as a Spark literal).\n",
    "    month_str = lit(month_match.group(0).lower()) if month_match else lit(None)\n",
    "\n",
    "    # Define a mapping between short Portuguese month names and their numerical equivalents.\n",
    "    month_number_map = {\n",
    "        'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "        'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "    }\n",
    "\n",
    "    # Convert the extracted month string to its numerical equivalent using the mapping.\n",
    "    # If the month string is not None, get the corresponding value from the map; otherwise, set to None.\n",
    "    month_number = when(month_str.isNotNull(),\n",
    "                    create_map([col for pair in [(lit(k), lit(v)) for k, v in month_number_map.items()] for col in pair]).getItem(month_str)\n",
    "                    ).otherwise(lit(None))\n",
    "\n",
    "    # Create a new DataFrame with normalized columns.\n",
    "\n",
    "        # Debugging: Print types of year and month_number\n",
    "    print(f\"Type of year: {type(year)}\")\n",
    "    print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "    try:\n",
    "        # Debugging: Print types of year and month_number (moved inside try)\n",
    "        print(f\"Inside try block:\")  # Indicate we're inside the try block\n",
    "        print(f\"Type of year: {type(year)}\")\n",
    "        print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "        normalized_df = df.withColumn(\"DataInput\", lit(datetime.now())) \\\n",
    "            .withColumn(\"Date\",\n",
    "                # Extract the date part using a regular expression and format it as YYYY/MM/DD.\n",
    "                # It handles cases where the day of the week (Portuguese or English) might be present.\n",
    "                when(regexp_extract(col(\"Extracted Text\"),\n",
    "                                r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? (\\d{2}) de ([a-z]{3})', 3\n",
    "                                ).isNotNull(),\n",
    "                    concat_ws(\"/\", year.cast(\"string\"), month_number.cast(\"string\"),\n",
    "                            regexp_extract(col(\"Extracted Text\"),\n",
    "                                        r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? (\\d{2}) de ([a-z]{3})', 2\n",
    "                                        ).cast(\"string\")\n",
    "                            )\n",
    "                    ).otherwise(concat_ws(\"/\", year.cast(\"string\"), month_number.cast(\"string\"), lit(None).cast(\"string\")).cast(\"string\"))\n",
    "                ) \\\n",
    "            .withColumn(\"Earnings\",\n",
    "                # Extract the earnings value (preceded by '€') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'€\\s*([\\d,.]+)', 1).cast(\"double\")) \\\n",
    "            .withColumn(\"Toll\",\n",
    "                # Check if any of the toll keywords are present in the 'Extracted Text'.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})'),  # Corrected line\n",
    "                    # If a toll keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"ServiceFee\",\n",
    "                # Check if any of the service fee keywords are present (preceded by '€').\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})'),  # Corrected line\n",
    "                    # If a service fee keyword is found, extract the numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Tip\",\n",
    "                # Check if any of the tip keywords are present.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})'),  # Corrected line\n",
    "                    # If a tip keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"StartTime\",\n",
    "                # Extract the start time in various formats (HH-MM, HH.MM, HH*MM) and standardize it to HH:MM.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})',\n",
    "                                    1).rlike(r'\\d{2}[-.:*]\\d{2}'),\n",
    "                    regexp_extract(col(\"Extracted Text\"), r'(\\d{2})[-\\.\\*](\\d{2})', 1) + \":\" + regexp_extract(\n",
    "                        col(\"Extracted Text\"), r'\\d{2}[-\\.\\*](\\d{2})', 2)\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"TotalTime\",\n",
    "                # Extract total time in the format 'digits min digits seg' and format it as MM:SS with leading zeros.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', 1).isNotNull(),\n",
    "                    concat_ws(\":\",\n",
    "                            # Left-pad the minutes with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min', 1), 2, \"0\"),\n",
    "                            # Left-pad the seconds with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*seg', 2), 2, \"0\")\n",
    "                            )\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Distance\",\n",
    "                # Extract the distance value (followed by ' km') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'([\\d,.]+)\\s*km', 1).cast(\"double\"))\n",
    "        return normalized_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_spark_dataframe: {e}\")\n",
    "        print(f\"Type of year (in except): {type(year)}\")\n",
    "        print(f\"Type of month_number (in except): {type(month_number)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None  # Or raise the exception if you want the program to stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully!\n",
      "Application name: toll_reconciliation_tool\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Confirming that the session is working\n",
    "if spark:\n",
    "    print(\"Spark session started successfully!\")\n",
    "    print(f\"Application name: {spark.sparkContext.appName}\")\n",
    "else:\n",
    "    print(\"Failed to start Spark session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Attempt a basic Hive operation\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metastore URI: default\n",
      "['bronze', 'default', 'gold', 'silver']\n",
      "The 'bronze' schema exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print Metastore URI for debugging\n",
    "print(f\"Metastore URI: {spark.conf.get('hive.metastore.uris', 'default')}\")  # Get with default to avoid error\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "print (schemas)\n",
    "\n",
    "\n",
    "# Tests to find the bronze layer\n",
    "if \"bronze\" not in schemas:\n",
    "    print(\"Error: The 'bronze' schema does not exist. Please create it.\")  # error for not finding the bronze layer\n",
    "\n",
    "else:\n",
    "    print(\"The 'bronze' schema exists.\")\n",
    "    # Switch to the bronze layer\n",
    "    spark.sql(f\"USE {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files added:\n",
      "C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecting Files:\n",
    "filepath_list = [] # Create an empty list\n",
    "add_files_to_list(filepath_list) # Call the function to add files and print the final list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for your CSV files (adjust based on your actual file structure)\n",
    "# Assuming your CSV has a single column named 'Extracted Text'\n",
    "schema = StructType([\n",
    "    StructField(\"Extracted Text\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b565a6-beba-4344-99f1-bdbc6e707fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of year: <class 'pyspark.sql.column.Column'>\n",
      "Type of month_number: <class 'pyspark.sql.column.Column'>\n",
      "Inside try block:\n",
      "Type of year: <class 'pyspark.sql.column.Column'>\n",
      "Type of month_number: <class 'pyspark.sql.column.Column'>\n",
      "\n",
      "--- Normalized DataFrame for file: C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv ---\n",
      "An error occurred while processing C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv: An error occurred while calling o176.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Renato-Laptop-LenovoIdeapad320-SSD executor driver): org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 1, but got 2.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 1, but got 2.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "All 1 files processed and loaded into 'bronze.tvde_earnings_history'.\n"
     ]
    }
   ],
   "source": [
    "# Process and load data using Spark\n",
    "for filepath in filepath_list:\n",
    "    try:\n",
    "        # Read the CSV file directly into a Spark DataFrame\n",
    "        df = spark.read.csv(filepath, encoding='utf-8', schema=schema)\n",
    "        # Normalize the Spark DataFrame using the normalize_spark_dataframe function\n",
    "        normalized_df = normalize_spark_dataframe(df, os.path.basename(filepath))\n",
    "\n",
    "        # Instead of writing to Delta, print the normalized DataFrame to the console\n",
    "        print(f\"\\n--- Normalized DataFrame for file: {filepath} ---\")\n",
    "        normalized_df.show(truncate=False) # Use truncate=False to show full content\n",
    "\n",
    "        \"\"\"\n",
    "        # Write the normalized Spark DataFrame to a Delta table\n",
    "        normalized_df.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(f'{layer}.{table}')\n",
    "        print(f\"Successfully processed and loaded {filepath} into {layer}.{table}\")\n",
    "        \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "\n",
    "print(f\"All {len(filepath_list)} files processed and loaded into '{layer}.{table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
