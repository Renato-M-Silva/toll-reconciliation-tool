{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or add data to tvde_earnings_history table in the bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import col, lit, regexp_extract, when, to_date, unix_timestamp, from_unixtime, concat_ws, lpad, create_map\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer and table\n",
    "layer = \"bronze\"\n",
    "table = \"tvde_earnings_history\"\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Functions (select_files, add_files_to_list, normalize_dataframe) ---\n",
    "def select_files():\n",
    "    \"\"\"Opens a file selection dialog and returns a list with the paths of the selected files.\"\"\"\n",
    "    root = tk.Tk()  # Create a Tkinter window\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select files\",  # Window title\n",
    "    )\n",
    "\n",
    "    return list(file_paths)  # Convert the returned tuple to a list\n",
    "\n",
    "def add_files_to_list(file_list):\n",
    "    \"\"\"Adds the paths of the selected files to the list.\"\"\"\n",
    "    selected_file_paths = select_files()\n",
    "\n",
    "    if selected_file_paths:  # Check if the user selected any files\n",
    "        file_list.extend(selected_file_paths)  # Add the paths to the list\n",
    "        print(\"Files added:\")\n",
    "        for file_path in selected_file_paths:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "def normalize_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a DataFrame according to specified rules.\"\"\"\n",
    "\n",
    "    df['Date'] = None\n",
    "    df['Earnings'] = None\n",
    "    df['Toll'] = None\n",
    "    df['ServiceFee'] = None\n",
    "    df['Tip'] = None\n",
    "    df['StartTime'] = None\n",
    "    df['TotalTime'] = None\n",
    "    df['Distance'] = None\n",
    "    df['DataInput'] = None\n",
    "\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE)\n",
    "\n",
    "    if year_match and month_match:\n",
    "        year = year_match.group(0)\n",
    "        month = month_match.group(0).lower()\n",
    "        month_number = {\n",
    "            'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "            'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "        }[month]\n",
    "    else:\n",
    "        year = None\n",
    "        month_number = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Extracted Text']\n",
    "\n",
    "        # include control columns\n",
    "        df.at[index, 'DataInput'] = datetime.now()\n",
    "\n",
    "        # Date\n",
    "        date_match = re.search(r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? \\d{2} de [a-z]{3}', text, re.IGNORECASE)\n",
    "        if date_match:\n",
    "            day_month = re.search(r'\\d{2} de [a-z]{3}', date_match.group(0), re.IGNORECASE).group(0)\n",
    "            day = re.search(r'\\d{2}', day_month).group(0)\n",
    "            if year and month_number:\n",
    "                df.at[index, 'Date'] = f\"{year}/{month_number}/{day}\"\n",
    "            else:\n",
    "                df.at[index, 'Date'] = f\"{day}\" \n",
    "        else:\n",
    "            df.at[index, 'Date'] = f'{year}/{month_number}/{day}'\n",
    "\n",
    "\n",
    "        # Earnings\n",
    "        earnings_match = re.search(r'€\\s*([\\d,.]+)', text)\n",
    "        if earnings_match:\n",
    "            df.at[index, 'Earnings'] = earnings_match.group(1).replace(',', '.')\n",
    "\n",
    "        # Toll\n",
    "        for word in toll_words:\n",
    "            toll_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if toll_match:\n",
    "                df.at[index, 'Toll'] = toll_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Service Fee\n",
    "        for word in service_fee_words:\n",
    "            service_match = re.search(r'€\\s*([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if service_match:\n",
    "                df.at[index, 'ServiceFee'] = service_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Tip\n",
    "        for word in tip_words:\n",
    "            tip_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if tip_match:\n",
    "                df.at[index, 'Tip'] = tip_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Start Time\n",
    "        start_time_match = re.search(r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})', text)\n",
    "        if start_time_match:\n",
    "            df.at[index, 'StartTime'] = start_time_match.group(1).replace('.', ':').replace('*', ':').replace('-', ':')\n",
    "\n",
    "        # Total Time\n",
    "        total_time_match = re.search(r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', text)\n",
    "        if total_time_match:\n",
    "            df.at[index, 'TotalTime'] = f\"{total_time_match.group(1)}:{total_time_match.group(2)}\"\n",
    "\n",
    "        # Distance\n",
    "        distance_match = re.search(r'([\\d,.]+)\\s*km', text)\n",
    "        if distance_match:\n",
    "            df.at[index, 'Distance'] = distance_match.group(1).replace(',', '.')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecting Files:\n",
    "filepath_list = [] # Create an empty list\n",
    "add_files_to_list(filepath_list) # Call the function to add files and print the final list\n",
    "pandas_dataframes_list = []  # List to store DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts each df in the 'dataframes_list', one for each CSV file.\n",
    "for filepath in filepath_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')  # Use utf-8 encoding\n",
    "        pandas_dataframes_list.append(df)\n",
    "        print(f\"Successfully read {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filepath}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: Empty CSV file - {filepath}\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Parsing error in {filepath}. Check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filepath}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# You can access each DataFrame like this:\n",
    "if pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,df in enumerate(pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {df.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(df.head())  # Print the first few rows of the first DataFrame\n",
    "else:\n",
    "    print(\"\\nNo CSV files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing Pandas DataFrame \n",
    "normalized_pandas_dataframes_list = [normalize_dataframe(df.copy(), os.path.basename(filepath)) for df, filepath in zip(pandas_dataframes_list, filepath_list)]\n",
    "\n",
    "\n",
    "# Now 'normalized_dataframes' contains the normalized DataFrames.\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)  # Increase terminal width (adjust as needed)\n",
    "\n",
    "# To view the first normalized DataFrame rows:\n",
    "if normalized_pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,ndf in enumerate(normalized_pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {ndf.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(ndf.head())  # Print the first few rows of each DataFrame\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo os tipos de dados no Pandas explicitamente\n",
    "for ndf in normalized_pandas_dataframes_list:\n",
    "    numeric_cols = ['Earnings', 'Toll', 'ServiceFee', 'Tip', 'Distance']\n",
    "    for col in numeric_cols:\n",
    "        ndf[col] = pd.to_numeric(ndf[col], errors='coerce')\n",
    "    ndf['DataInput'] = pd.to_datetime(ndf['DataInput'])\n",
    "    string_cols = ['Extracted Text', 'Date', 'StartTime', 'TotalTime']\n",
    "    for col in string_cols:\n",
    "        ndf[col] = ndf[col].astype(str)\n",
    "    print(ndf.dtypes) # Verifique os tipos após a conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Confirming that the session is working\n",
    "if spark:\n",
    "    print(\"Spark session started successfully!\")\n",
    "    print(f\"Application name: {spark.sparkContext.appName}\")\n",
    "else:\n",
    "    print(\"Failed to start Spark session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt a basic Hive operation\n",
    "try:\n",
    "    # Show a list of layers/schemas/databases in the spark-warehouse\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "\n",
    "if \"bronze\" not in schemas:\n",
    "    print(\"Error: The 'bronze' schema does not exist. Please create it.\")  # error for not finding the bronze layer\n",
    "else:\n",
    "    print(\"The 'bronze' schema exists.\")\n",
    "    # Switch to the bronze layer\n",
    "    spark.sql(f\"USE {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert each Pandas DataFrame in the list to a Spark DataFrame\n",
    "spark_dataframes_list = [spark.createDataFrame(ndf) for ndf in normalized_pandas_dataframes_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To view the first normalized Spark DataFrame rows:\n",
    "if spark_dataframes_list:  # Check if the list is not empty\n",
    "    for i, sdf in enumerate(spark_dataframes_list):\n",
    "        print(f\"Visualizando DataFrame número: {i + 1} da lista\")\n",
    "        print(\"\\nPrimeiras 5 linhas:\")\n",
    "        sdf.show(5)\n",
    "        print(\"-\" * 30)  # Separador para melhor visualizaçã\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sdf in spark_dataframes_list:\n",
    "    try:\n",
    "        # Write the normalized Spark DataFrame to a Delta table\n",
    "        sdf.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(f'{layer}.{table}')\n",
    "        print(f\"Successfully processed and loaded {filepath} into {layer}.{table}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"All {len(filepath_list)} files processed (attempted) and loaded into '{layer}.{table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
