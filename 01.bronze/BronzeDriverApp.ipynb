{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or add data to tvde_earnings_history table in the bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, regexp_extract, when, to_date, unix_timestamp, from_unixtime, concat_ws, lpad, create_map\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark warehouse to: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n",
      "Changed current working directory to: C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "# layer and table\n",
    "layer = \"bronze\"\n",
    "table = \"tvde_earnings_history\"\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Functions (select_files, add_files_to_list, normalize_spark_dataframe) ---\n",
    "def select_files():\n",
    "    \"\"\"Opens a file selection dialog and returns a list with the paths of the selected files.\"\"\"\n",
    "    root = tk.Tk() # Create a Tkinter window\n",
    "    root.withdraw() # Hide the main window\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select files\",  # Window title\n",
    "    )\n",
    "    return list(file_paths) # Convert the returned tuple to a list\n",
    "\n",
    "def add_files_to_list(file_list):\n",
    "    \"\"\"Adds the paths of the selected files to the list.\"\"\"\n",
    "    selected_file_paths = select_files()\n",
    "    if selected_file_paths: # Check if the user selected any files\n",
    "        file_list.extend(selected_file_paths)  # Add the paths to the list\n",
    "        print(\"Files added:\")\n",
    "        for file_path in selected_file_paths:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "\n",
    "def normalize_spark_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a Spark DataFrame according to specified rules, using keyword lists.\"\"\"\n",
    "    \n",
    "    # Define lists of keywords for toll, service fee, and tip\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(\n",
    "        r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Extract the year if found, otherwise set to None (as a Spark literal).\n",
    "    year = lit(year_match.group(0)) if year_match else lit(None)\n",
    "\n",
    "    # Extract the lowercase month string if found, otherwise set to None (as a Spark literal).\n",
    "    month_str = lit(month_match.group(0).lower()) if month_match else lit(None)\n",
    "\n",
    "    # Define a mapping between short Portuguese month names and their numerical equivalents.\n",
    "    month_number_map = {\n",
    "        'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "        'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "    }\n",
    "\n",
    "    # Convert the extracted month string to its numerical equivalent using the mapping.\n",
    "    # If the month string is not None, get the corresponding value from the map; otherwise, set to None.\n",
    "    month_number = when(month_str.isNotNull(),\n",
    "                    create_map([col for pair in [(lit(k), lit(v)) for k, v in month_number_map.items()] for col in pair]).getItem(month_str)\n",
    "                    ).otherwise(lit(None))\n",
    "\n",
    "    # Create a new DataFrame with normalized columns.\n",
    "\n",
    "        # Debugging: Print types of year and month_number\n",
    "    print(f\"Type of year: {type(year)}\")\n",
    "    print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "    try:\n",
    "        # Debugging: Print types of year and month_number (moved inside try)\n",
    "        print(f\"Inside try block:\")  # Indicate we're inside the try block\n",
    "        print(f\"Type of year: {type(year)}\")\n",
    "        print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "        normalized_df = df.withColumn(\"DataInput\", lit(datetime.now())) \\\n",
    "            .withColumn(\"Date\",\n",
    "                # Extract the date part using a regular expression and format it as YYYY/MM/DD.\n",
    "                # It handles cases where the day of the week (Portuguese or English) might be present.\n",
    "                when(\n",
    "                    # Check if the whole pattern matches\n",
    "                    col(\"Extracted Text\").rlike(r'(?i)(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,?\\s*\\d{2}\\s*de\\s*[a-z]{3}'),\n",
    "                    concat_ws(\n",
    "                        \"/\",\n",
    "                        year.cast(\"string\"),\n",
    "                        month_number.cast(\"string\"),\n",
    "                        # Extract the day (group 2)\n",
    "                        regexp_extract(col(\"Extracted Text\"), r'(?i)(?:seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,?\\s*(\\d{2})\\s*de\\s*[a-z]{3}', 1).cast(\"string\")\n",
    "                    )\n",
    "                ).otherwise(\n",
    "                    # Handle cases where the date pattern is not found\n",
    "                    lit(None).cast(\"string\")\n",
    "                ) \n",
    "            )\\\n",
    "            .withColumn(\"Earnings\",\n",
    "                # Extract the earnings value (preceded by '€') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'€\\s*([\\d,.]+)', 1).cast(\"double\")) \\\n",
    "            .withColumn(\"Toll\",\n",
    "                # Check if any of the toll keywords are present in the 'Extracted Text'.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})'),  # Corrected line\n",
    "                    # If a toll keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"ServiceFee\",\n",
    "                # Check if any of the service fee keywords are present (preceded by '€').\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})'),  # Corrected line\n",
    "                    # If a service fee keyword is found, extract the numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Tip\",\n",
    "                # Check if any of the tip keywords are present.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})'),  # Corrected line\n",
    "                    # If a tip keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"StartTime\",\n",
    "                # Extract the start time in various formats (HH-MM, HH.MM, HH*MM) and standardize it to HH:MM.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})',\n",
    "                                    1).rlike(r'\\d{2}[-.:*]\\d{2}'),\n",
    "                    regexp_extract(col(\"Extracted Text\"), r'(\\d{2})[-\\.\\*](\\d{2})', 1) + \":\" + regexp_extract(\n",
    "                        col(\"Extracted Text\"), r'\\d{2}[-\\.\\*](\\d{2})', 2)\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"TotalTime\",\n",
    "                # Extract total time in the format 'digits min digits seg' and format it as MM:SS with leading zeros.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', 1).isNotNull(),\n",
    "                    concat_ws(\":\",\n",
    "                            # Left-pad the minutes with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min', 1), 2, \"0\"),\n",
    "                            # Left-pad the seconds with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*seg', 2), 2, \"0\")\n",
    "                            )\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Distance\",\n",
    "                # Extract the distance value (followed by ' km') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'([\\d,.]+)\\s*km', 1).cast(\"double\"))\n",
    "        return normalized_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_spark_dataframe: {e}\")\n",
    "        print(f\"Type of year (in except): {type(year)}\")\n",
    "        print(f\"Type of month_number (in except): {type(month_number)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None  # Or raise the exception if you want the program to stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully!\n",
      "Application name: toll_reconciliation_tool\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Confirming that the session is working\n",
    "if spark:\n",
    "    print(\"Spark session started successfully!\")\n",
    "    print(f\"Application name: {spark.sparkContext.appName}\")\n",
    "else:\n",
    "    print(\"Failed to start Spark session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Attempt a basic Hive operation\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metastore URI: default\n",
      "['bronze', 'default', 'gold', 'silver']\n",
      "The 'bronze' schema exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print Metastore URI for debugging\n",
    "print(f\"Metastore URI: {spark.conf.get('hive.metastore.uris', 'default')}\")  # Get with default to avoid error\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "print (schemas)\n",
    "\n",
    "\n",
    "# Tests to find the bronze layer\n",
    "if \"bronze\" not in schemas:\n",
    "    print(\"Error: The 'bronze' schema does not exist. Please create it.\")  # error for not finding the bronze layer\n",
    "\n",
    "else:\n",
    "    print(\"The 'bronze' schema exists.\")\n",
    "    # Switch to the bronze layer\n",
    "    spark.sql(f\"USE {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files added:\n",
      "C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecting Files:\n",
    "filepath_list = [] # Create an empty list\n",
    "add_files_to_list(filepath_list) # Call the function to add files and print the final list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for your CSV files (adjust based on your actual file structure)\n",
    "# Assuming your CSV has a single column named 'Extracted Text'\n",
    "schema = StructType([\n",
    "    StructField(\"Extracted Text\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spark_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a Spark DataFrame according to specified rules, using keyword lists.\"\"\"\n",
    "\n",
    "    # Define lists of keywords for toll, service fee, and tip\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(\n",
    "        r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Extract the year if found, otherwise set to None (as a Spark literal).\n",
    "    year = lit(year_match.group(0)) if year_match else lit(None)\n",
    "\n",
    "    # Extract the lowercase month string if found, otherwise set to None (as a Spark literal).\n",
    "    month_str = lit(month_match.group(0).lower()) if month_match else lit(None)\n",
    "\n",
    "    # Define a mapping between short Portuguese month names and their numerical equivalents.\n",
    "    month_number_map = {\n",
    "        'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "        'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "    }\n",
    "\n",
    "    # Convert the extracted month string to its numerical equivalent using the mapping.\n",
    "    # If the month string is not None, get the corresponding value from the map; otherwise, set to None.\n",
    "    month_number = when(month_str.isNotNull(),\n",
    "                    create_map([col for pair in [(lit(k), lit(v)) for k, v in month_number_map.items()] for col in pair]).getItem(month_str)\n",
    "                    ).otherwise(lit(None))\n",
    "\n",
    "    # Create a new DataFrame with normalized columns.\n",
    "\n",
    "    # Debugging: Print types of year and month_number\n",
    "    print(f\"Type of year: {type(year)}\")\n",
    "    print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "    try:\n",
    "        # Debugging: Print types of year and month_number (moved inside try)\n",
    "        print(f\"Inside try block:\")  # Indicate we're inside the try block\n",
    "        print(f\"Type of year: {type(year)}\")\n",
    "        print(f\"Type of month_number: {type(month_number)}\")\n",
    "\n",
    "        normalized_df = df.withColumn(\"DataInput\", lit(datetime.now())) \\\n",
    "            .withColumn(\"Date\",\n",
    "                when(\n",
    "                    # First, extract the full date string\n",
    "                    regexp_extract(col(\"Extracted Text\"), r'(?i)(?:seg|ter|qua|qui|sex|s[aá]b|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,?\\s*(\\d{2}\\s*de\\s*[a-z]{3})', 1).isNotNull(),\n",
    "                    concat_ws(\n",
    "                        \"/\",\n",
    "                        year.cast(\"string\"),\n",
    "                        month_number.cast(\"string\"),\n",
    "                        # Then, extract *only* the day from the extracted date string\n",
    "                        regexp_extract(regexp_extract(col(\"Extracted Text\"), r'(?i)(?:seg|ter|qua|qui|sex|s[aá]b|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,?\\s*(\\d{2})\\s*de\\s*[a-z]{3}', 1), r'(\\d{2})', 1).cast(\"string\")\n",
    "                    )\n",
    "                ).otherwise(\n",
    "                    lit(None).cast(\"string\")\n",
    "                )\n",
    "            ) \\\n",
    "            .withColumn(\"Earnings\",\n",
    "                # Extract the earnings value (preceded by '€') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'€\\s*([\\d,.]+)', 1).cast(\"double\")) \\\n",
    "            .withColumn(\"Toll\",\n",
    "                # Check if any of the toll keywords are present in the 'Extracted Text'.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})'),  # Corrected line\n",
    "                    # If a toll keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(toll_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"ServiceFee\",\n",
    "                # Check if any of the service fee keywords are present (preceded by '€').\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})'),  # Corrected line\n",
    "                    # If a service fee keyword is found, extract the numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)€\\s*([\\d,.]+)\\s*({\"|\".join(service_fee_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Tip\",\n",
    "                # Check if any of the tip keywords are present.\n",
    "                when(col(\"Extracted Text\").rlike(rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})'),  # Corrected line\n",
    "                    # If a tip keyword is found, extract the corresponding numeric value and cast it to a double (case-insensitive).\n",
    "                    regexp_extract(col(\"Extracted Text\"), rf'(?i)([\\d,.]+)\\s*({\"|\".join(tip_words)})', 1).cast(\"double\")  # Corrected line\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"StartTime\",\n",
    "                # Extract the start time in various formats (HH-MM, HH.MM, HH*MM) and standardize it to HH:MM.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})',\n",
    "                                    1).rlike(r'\\d{2}[-.:*]\\d{2}'),\n",
    "                    regexp_extract(col(\"Extracted Text\"), r'(\\d{2})[-\\.\\*](\\d{2})', 1) + \":\" + regexp_extract(\n",
    "                        col(\"Extracted Text\"), r'\\d{2}[-\\.\\*](\\d{2})', 2)\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"TotalTime\",\n",
    "                # Extract total time in the format 'digits min digits seg' and format it as MM:SS with leading zeros.\n",
    "                when(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', 1).isNotNull(),\n",
    "                    concat_ws(\":\",\n",
    "                            # Left-pad the minutes with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*min', 1), 2, \"0\"),\n",
    "                            # Left-pad the seconds with '0' to ensure two digits.\n",
    "                            lpad(regexp_extract(col(\"Extracted Text\"), r'(\\d+)\\s*seg', 2), 2, \"0\")\n",
    "                            )\n",
    "                    ).otherwise(lit(None))) \\\n",
    "            .withColumn(\"Distance\",\n",
    "                # Extract the distance value (followed by ' km') and cast it to a double.\n",
    "                regexp_extract(col(\"Extracted Text\"), r'([\\d,.]+)\\s*km', 1).cast(\"double\"))\n",
    "        return normalized_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_spark_dataframe: {e}\")\n",
    "        print(f\"Type of year (in except): {type(year)}\")\n",
    "        print(f\"Type of month_number (in except): {type(month_number)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback\n",
    "        return None  # Or raise the exception if you want the program to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample data from Extracted Text column ---\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Extracted Text                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Extracted Text                                                                                                                                                                                                                                                                             |\n",
      "|sex , 29 de dez                                                                                                                                                                                                                                                                            |\n",
      "|€ 2,60 1l-46 Taxa de serviço                                                                                                                                                                                                                                                               |\n",
      "|€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google|\n",
      "|€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT                                                                            |\n",
      "|€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT                                                                          |\n",
      "|€0,00 19.37 UberX Saver 0 usuario cancelou rrábida Apartment nique Hosts Google Map data 02025 InstGeoarNacional Praça Diogo de Macedo; 4400-480 Vila Nova de Gaia, Praça Diogo de Macedo; 4400-480 Vila Nova de Gaia,                                                                     |\n",
      "|€4,88 UberX Saver . 16 min 58 segundos 19.26 6.67 km A28 Porto Campanha A43 Ponte Luís N209] Google Map data 02025 InstGeogr Nacional 4000-059 Porto; PT R Manuel Moreira Barros, 4400-346 Vila Nova de Gaia PT                                                                            |\n",
      "|€2,84 UberX Saver 6 min segundos . 1,69 19.00 km Livraria Lello Jardins do BOLHAO Palácio Mira de Cristal Google Lap dala 02025 InstGeogr Nacional Praça da Liberdade, 4000-322 Porto; PT R de São Filipe de 4050-546 Porto; PT Nery;                                                      |\n",
      "|€ 5,43 UberX Saver . 23 min 34 segundos 18.35 703 km Ermesinde Valongo Matosink Rio Tinto Google_ Map data 02025 Google InstGeogr Nacional R Sara Afonso; 4460-841 Sra. da Hora, PT R da Galeria de Paris, 4000 Porto PT                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:460: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of year: <class 'pyspark.sql.column.Column'>\n",
      "Type of month_number: <class 'pyspark.sql.column.Column'>\n",
      "Inside try block:\n",
      "Type of year: <class 'pyspark.sql.column.Column'>\n",
      "Type of month_number: <class 'pyspark.sql.column.Column'>\n",
      "\n",
      "--- Normalized DataFrame for file: C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv ---\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Extracted Text                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Extracted Text                                                                                                                                                                                                                                                                             |\n",
      "|sex , 29 de dez                                                                                                                                                                                                                                                                            |\n",
      "|€ 2,60 1l-46 Taxa de serviço                                                                                                                                                                                                                                                               |\n",
      "|€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google|\n",
      "|€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT                                                                            |\n",
      "|€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT                                                                          |\n",
      "|€0,00 19.37 UberX Saver 0 usuario cancelou rrábida Apartment nique Hosts Google Map data 02025 InstGeoarNacional Praça Diogo de Macedo; 4400-480 Vila Nova de Gaia, Praça Diogo de Macedo; 4400-480 Vila Nova de Gaia,                                                                     |\n",
      "|€4,88 UberX Saver . 16 min 58 segundos 19.26 6.67 km A28 Porto Campanha A43 Ponte Luís N209] Google Map data 02025 InstGeogr Nacional 4000-059 Porto; PT R Manuel Moreira Barros, 4400-346 Vila Nova de Gaia PT                                                                            |\n",
      "|€2,84 UberX Saver 6 min segundos . 1,69 19.00 km Livraria Lello Jardins do BOLHAO Palácio Mira de Cristal Google Lap dala 02025 InstGeogr Nacional Praça da Liberdade, 4000-322 Porto; PT R de São Filipe de 4050-546 Porto; PT Nery;                                                      |\n",
      "|€ 5,43 UberX Saver . 23 min 34 segundos 18.35 703 km Ermesinde Valongo Matosink Rio Tinto Google_ Map data 02025 Google InstGeogr Nacional R Sara Afonso; 4460-841 Sra. da Hora, PT R da Galeria de Paris, 4000 Porto PT                                                                   |\n",
      "|€6,79 16-50 UberX . 21 min segundos 10.44 km Lousada Google_ Map data 02025 GoogleInstGeogr Nacional 4470-558 Maia, PT R Sara Afonso; 4460-841 Sra, da Hora, PT                                                                                                                            |\n",
      "|€ 12,08 Aumentou UberX Saver - 32 min 43 segundos 16.04 23.95 km P € 0,45 Pedágio Portr Google_ Map data 2025 Google InstGeogr Nacional Av Padre Jorge Duarte; 4430-946 Vila Nova de Gaia, Via do Aeroporto; 4470 Maia, PT                                                                 |\n",
      "|€11,33 Aumentou UberX Saver . 38 min 40 segundos 15-21 22.59 km P €0,15 Pedágio Maia Google Map data 02025 Gopgle InstGeogrNacional 4460 Sra, da Hora, PT R. Padre Manuel Valente Pinho Leão; 4430-999 Vila Ni de Gaia, PT                                                                 |\n",
      "|€3,03 UberX Saver . 15 min segundos 15.00 4.15 km Leça da Palmeira São Mamede W15-1 de Intesta Matosinhos Rio Tinto Porto Google_ data 92025 Gooole;InstGeogr Nacional R Carlos Dubini, 4150-188 Porto; PT 4460 Sra, da Hora, PT Mop                                                       |\n",
      "|€2,84 UberX Saver 8 min 29 segundos 14.41 3,67 km boavista Foz do Douro Porto Google_ Map date o2025 InstGeogr Nacional R Dom João de Castro; 4150-417 Porto; PT Via Panorâmica Edgar Cardoso; 4150-564 Porto; PT                                                                          |\n",
      "|qui , 28 de dez                                                                                                                                                                                                                                                                            |\n",
      "|€ 9,56 UberX . 15 min 43 segundos 13.59 21-58 km A11 Lousada Maia Google_ Map data 02025 Gopgle InstGeogr Nacional 4470-558 Moreira, PT R. da Arrábida, 4150-001 Porto; PT                                                                                                                 |\n",
      "|€3,56 UberX Saver . 13 min 13 segundos 21*08 712 km Angeiras Pinheiro IShopping Lavra Vila Nova Google ap data 02025 Largo Dr, Fernando Aroso; 4455-130 Lavra, PT R de Lagielas; 4470-558 Vila Nova da Telha; PT Vilar '                                                                   |\n",
      "|€ 6,80 UberX Saver . 15 min 28 segundos 20-49 11.79 km Lousada Google_ Map data 02025 GooaleInstGeoar Nacional R. José Joaquim Gomes da Silva, 4450-100 Matosinh( PT Rde Avilhoso; 4455 Lavra; PT                                                                                          |\n",
      "|€ 2,88 UberX Saver 6 min 51 segundos 20-35 3,49 km Matosinhos NorteShopping Parque da Quinta do Cidade Covelo Google_ do Porto Map data 02025 InstGeogr Nacional R Fonte da Moura, 4100-256 Porto, PT R de Dr. Eduardo Torres, 4450-115 Matosinhos, PT                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "All 1 files processed (attempted) and loaded into 'bronze.tvde_earnings_history'.\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_list:\n",
    "    try:\n",
    "        # Read the CSV file directly into a Spark DataFrame\n",
    "        df = spark.read.csv(filepath, encoding='utf-8', schema=schema)\n",
    "\n",
    "        # Print some sample data for debugging\n",
    "        print(\"\\n--- Sample data from Extracted Text column ---\")\n",
    "        df.select(\"Extracted Text\").show(10, truncate=False)  # Show the first 10 rows\n",
    "\n",
    "        # Normalize the Spark DataFrame using the normalize_spark_dataframe function\n",
    "        normalized_df = normalize_spark_dataframe(df, os.path.basename(filepath))\n",
    "\n",
    "        if normalized_df is not None:  # Only proceed if normalization was successful\n",
    "            print(f\"\\n--- Normalized DataFrame for file: {filepath} ---\")\n",
    "            normalized_df.select(\"Extracted Text\").show(truncate=False)\n",
    "\n",
    "            \"\"\"\n",
    "            # Write the normalized Spark DataFrame to a Delta table\n",
    "            normalized_df.write \\\n",
    "                .format('delta') \\\n",
    "                .mode('append') \\\n",
    "                .option('mergeSchema', 'true') \\\n",
    "                .saveAsTable(f'{layer}.{table}')\n",
    "            print(f\"Successfully processed and loaded {filepath} into {layer}.{table}\")\n",
    "            \"\"\"\n",
    "\n",
    "        else:\n",
    "            print(f\"Normalization failed for {filepath}, skipping Delta write.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"All {len(filepath_list)} files processed (attempted) and loaded into '{layer}.{table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
