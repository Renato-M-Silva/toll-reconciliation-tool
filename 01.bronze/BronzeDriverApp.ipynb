{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or add data to tvde_earnings_history table in the bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import col, lit, regexp_extract, when, to_date, unix_timestamp, from_unixtime, concat_ws, lpad, create_map\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark warehouse to: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n",
      "Changed current working directory to: C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "# layer and table\n",
    "layer = \"bronze\"\n",
    "table = \"tvde_earnings_history\"\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Functions (select_files, add_files_to_list, normalize_dataframe) ---\n",
    "def select_files():\n",
    "    \"\"\"Opens a file selection dialog and returns a list with the paths of the selected files.\"\"\"\n",
    "    root = tk.Tk()  # Create a Tkinter window\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select files\",  # Window title\n",
    "    )\n",
    "\n",
    "    return list(file_paths)  # Convert the returned tuple to a list\n",
    "\n",
    "def add_files_to_list(file_list):\n",
    "    \"\"\"Adds the paths of the selected files to the list.\"\"\"\n",
    "    selected_file_paths = select_files()\n",
    "\n",
    "    if selected_file_paths:  # Check if the user selected any files\n",
    "        file_list.extend(selected_file_paths)  # Add the paths to the list\n",
    "        print(\"Files added:\")\n",
    "        for file_path in selected_file_paths:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "def normalize_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a DataFrame according to specified rules.\"\"\"\n",
    "\n",
    "    df['Date'] = None\n",
    "    df['Earnings'] = None\n",
    "    df['Toll'] = None\n",
    "    df['ServiceFee'] = None\n",
    "    df['Tip'] = None\n",
    "    df['StartTime'] = None\n",
    "    df['TotalTime'] = None\n",
    "    df['Distance'] = None\n",
    "    df['DataInput'] = None\n",
    "\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE)\n",
    "\n",
    "    if year_match and month_match:\n",
    "        year = year_match.group(0)\n",
    "        month = month_match.group(0).lower()\n",
    "        month_number = {\n",
    "            'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "            'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "        }[month]\n",
    "    else:\n",
    "        year = None\n",
    "        month_number = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Extracted Text']\n",
    "\n",
    "        # include control columns\n",
    "        df.at[index, 'DataInput'] = datetime.now()\n",
    "\n",
    "        # Date\n",
    "        date_match = re.search(r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? \\d{2} de [a-z]{3}', text, re.IGNORECASE)\n",
    "        if date_match:\n",
    "            day_month = re.search(r'\\d{2} de [a-z]{3}', date_match.group(0), re.IGNORECASE).group(0)\n",
    "            day = re.search(r'\\d{2}', day_month).group(0)\n",
    "            if year and month_number:\n",
    "                df.at[index, 'Date'] = f\"{year}/{month_number}/{day}\"\n",
    "            else:\n",
    "                df.at[index, 'Date'] = f\"{day}\" \n",
    "        else:\n",
    "            df.at[index, 'Date'] = f'{year}/{month_number}/{day}'\n",
    "\n",
    "\n",
    "        # Earnings\n",
    "        earnings_match = re.search(r'€\\s*([\\d,.]+)', text)\n",
    "        if earnings_match:\n",
    "            df.at[index, 'Earnings'] = earnings_match.group(1).replace(',', '.')\n",
    "\n",
    "        # Toll\n",
    "        for word in toll_words:\n",
    "            toll_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if toll_match:\n",
    "                df.at[index, 'Toll'] = toll_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Service Fee\n",
    "        for word in service_fee_words:\n",
    "            service_match = re.search(r'€\\s*([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if service_match:\n",
    "                df.at[index, 'ServiceFee'] = service_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Tip\n",
    "        for word in tip_words:\n",
    "            tip_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if tip_match:\n",
    "                df.at[index, 'Tip'] = tip_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Start Time\n",
    "        start_time_match = re.search(r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})', text)\n",
    "        if start_time_match:\n",
    "            df.at[index, 'StartTime'] = start_time_match.group(1).replace('.', ':').replace('*', ':').replace('-', ':')\n",
    "\n",
    "        # Total Time\n",
    "        total_time_match = re.search(r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', text)\n",
    "        if total_time_match:\n",
    "            df.at[index, 'TotalTime'] = f\"{total_time_match.group(1)}:{total_time_match.group(2)}\"\n",
    "\n",
    "        # Distance\n",
    "        distance_match = re.search(r'([\\d,.]+)\\s*km', text)\n",
    "        if distance_match:\n",
    "            df.at[index, 'Distance'] = distance_match.group(1).replace(',', '.')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files added:\n",
      "C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecting Files:\n",
    "filepath_list = [] # Create an empty list\n",
    "add_files_to_list(filepath_list) # Call the function to add files and print the final list\n",
    "pandas_dataframes_list = []  # List to store DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n",
      "Number of rows in (252, 1)\n",
      "\n",
      "First few rows of the 1th DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Extracted Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2098d57d-4e56-4594-b1b9-12262e4be41d",
       "rows": [
        [
         "0",
         "sex , 29 de dez"
        ],
        [
         "1",
         "€ 2,60 1l-46 Taxa de serviço"
        ],
        [
         "2",
         "€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google"
        ],
        [
         "3",
         "€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT"
        ],
        [
         "4",
         "€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sex , 29 de dez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€ 2,60 1l-46 Taxa de serviço</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>€3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€5,22 UberX Saver . 18 min 46 segundos 20-12 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>€4,37 UberX Saver ' 18 min 1l segundos 19.45 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Extracted Text\n",
       "0                                    sex , 29 de dez\n",
       "1                       € 2,60 1l-46 Taxa de serviço\n",
       "2  €3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...\n",
       "3  €5,22 UberX Saver . 18 min 46 segundos 20-12 8...\n",
       "4  €4,37 UberX Saver ' 18 min 1l segundos 19.45 6..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# puts each df in the 'dataframes_list', one for each CSV file.\n",
    "for filepath in filepath_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')  # Use utf-8 encoding\n",
    "        pandas_dataframes_list.append(df)\n",
    "        print(f\"Successfully read {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filepath}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: Empty CSV file - {filepath}\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Parsing error in {filepath}. Check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filepath}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# You can access each DataFrame like this:\n",
    "if pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,df in enumerate(pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {df.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(df.head())  # Print the first few rows of the first DataFrame\n",
    "else:\n",
    "    print(\"\\nNo CSV files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in (252, 10)\n",
      "\n",
      "First few rows of the 1th DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Extracted Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Earnings",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Toll",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ServiceFee",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Tip",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "StartTime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TotalTime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Distance",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DataInput",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b62d507b-711d-438d-8804-ad7e9a57ba79",
       "rows": [
        [
         "0",
         "sex , 29 de dez",
         "2023/12/29",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "2025-05-03 22:06:41.212390"
        ],
        [
         "1",
         "€ 2,60 1l-46 Taxa de serviço",
         "2023/12/29",
         "2.60",
         null,
         null,
         null,
         null,
         null,
         null,
         "2025-05-03 22:06:41.215383"
        ],
        [
         "2",
         "€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google",
         "2023/12/29",
         "3.23",
         null,
         null,
         "0.50",
         "21:23",
         "6:22",
         "138",
         "2025-05-03 22:06:41.216381"
        ],
        [
         "3",
         "€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT",
         "2023/12/29",
         "5.22",
         null,
         null,
         null,
         "20:12",
         "18:46",
         "8.22",
         "2025-05-03 22:06:41.216381"
        ],
        [
         "4",
         "€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT",
         "2023/12/29",
         "4.37",
         null,
         null,
         null,
         "19:45",
         null,
         "6.89",
         "2025-05-03 22:06:41.217378"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Toll</th>\n",
       "      <th>ServiceFee</th>\n",
       "      <th>Tip</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>TotalTime</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DataInput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sex , 29 de dez</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-03 22:06:41.212390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€ 2,60 1l-46 Taxa de serviço</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>2.60</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-05-03 22:06:41.215383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>€3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>3.23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.50</td>\n",
       "      <td>21:23</td>\n",
       "      <td>6:22</td>\n",
       "      <td>138</td>\n",
       "      <td>2025-05-03 22:06:41.216381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€5,22 UberX Saver . 18 min 46 segundos 20-12 8...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>5.22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20:12</td>\n",
       "      <td>18:46</td>\n",
       "      <td>8.22</td>\n",
       "      <td>2025-05-03 22:06:41.216381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>€4,37 UberX Saver ' 18 min 1l segundos 19.45 6...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>4.37</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19:45</td>\n",
       "      <td>None</td>\n",
       "      <td>6.89</td>\n",
       "      <td>2025-05-03 22:06:41.217378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Extracted Text        Date Earnings  Toll ServiceFee   Tip StartTime TotalTime Distance                   DataInput\n",
       "0                                    sex , 29 de dez  2023/12/29     None  None       None  None      None      None     None  2025-05-03 22:06:41.212390\n",
       "1                       € 2,60 1l-46 Taxa de serviço  2023/12/29     2.60  None       None  None      None      None     None  2025-05-03 22:06:41.215383\n",
       "2  €3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...  2023/12/29     3.23  None       None  0.50     21:23      6:22      138  2025-05-03 22:06:41.216381\n",
       "3  €5,22 UberX Saver . 18 min 46 segundos 20-12 8...  2023/12/29     5.22  None       None  None     20:12     18:46     8.22  2025-05-03 22:06:41.216381\n",
       "4  €4,37 UberX Saver ' 18 min 1l segundos 19.45 6...  2023/12/29     4.37  None       None  None     19:45      None     6.89  2025-05-03 22:06:41.217378"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalizing Pandas DataFrame \n",
    "normalized_pandas_dataframes_list = [normalize_dataframe(df.copy(), os.path.basename(filepath)) for df, filepath in zip(pandas_dataframes_list, filepath_list)]\n",
    "\n",
    "\n",
    "# Now 'normalized_dataframes' contains the normalized DataFrames.\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)  # Increase terminal width (adjust as needed)\n",
    "\n",
    "# To view the first normalized DataFrame rows:\n",
    "if normalized_pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,ndf in enumerate(normalized_pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {ndf.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(ndf.head())  # Print the first few rows of each DataFrame\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text            object\n",
      "Date                      object\n",
      "Earnings                 float64\n",
      "Toll                     float64\n",
      "ServiceFee               float64\n",
      "Tip                      float64\n",
      "StartTime                 object\n",
      "TotalTime                 object\n",
      "Distance                 float64\n",
      "DataInput         datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertendo os tipos de dados no Pandas explicitamente\n",
    "for ndf in normalized_pandas_dataframes_list:\n",
    "    numeric_cols = ['Earnings', 'Toll', 'ServiceFee', 'Tip', 'Distance']\n",
    "    for col in numeric_cols:\n",
    "        ndf[col] = pd.to_numeric(ndf[col], errors='coerce')\n",
    "    ndf['DataInput'] = pd.to_datetime(ndf['DataInput'])\n",
    "    string_cols = ['Extracted Text', 'Date', 'StartTime', 'TotalTime']\n",
    "    for col in string_cols:\n",
    "        ndf[col] = ndf[col].astype(str)\n",
    "    print(ndf.dtypes) # Verifique os tipos após a conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully!\n",
      "Application name: toll_reconciliation_tool\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Confirming that the session is working\n",
    "if spark:\n",
    "    print(\"Spark session started successfully!\")\n",
    "    print(f\"Application name: {spark.sparkContext.appName}\")\n",
    "else:\n",
    "    print(\"Failed to start Spark session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "Selecting the 'bronze' schema.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Show a list of layers/schemas/databases in the spark-warehouse\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "\n",
    "if \"bronze\" not in schemas:\n",
    "    print(\"Error: The 'bronze' schema does not exist. Please create it.\")  # error for not finding the bronze layer\n",
    "else:\n",
    "    print(\"Selecting the 'bronze' schema.\")\n",
    "    # Switch to the bronze layer\n",
    "    spark.sql(f\"USE {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert each Pandas DataFrame in the list to a Spark DataFrame\n",
    "spark_dataframes_list = [spark.createDataFrame(ndf) for ndf in normalized_pandas_dataframes_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizando DataFrame número: 1 da lista\n",
      "Schema:\n",
      "root\n",
      " |-- Extracted Text: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Earnings: double (nullable = true)\n",
      " |-- Toll: double (nullable = true)\n",
      " |-- ServiceFee: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- TotalTime: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- DataInput: timestamp (nullable = true)\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To view the first normalized Spark DataFrame rows:\n",
    "if spark_dataframes_list:  # Check if the list is not empty\n",
    "    for i, sdf in enumerate(spark_dataframes_list):\n",
    "        print(f\"Visualizando DataFrame número: {i + 1} da lista\")\n",
    "        print(\"Schema:\")\n",
    "        sdf.printSchema()\n",
    "        #print(\"\\nPrimeiras 5 linhas:\")\n",
    "        #sdf.head(5)\n",
    "        print(\"-\" * 30)  # Separador para melhor visualizaçã\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtendo informações de versão:\n",
      "Versão do Python: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "Versão do Spark: 3.5.5\n",
      "Scala não encontrado. Certifique-se de que o Scala está no seu PATH e que a variável de ambiente SCALA_HOME está definida corretamente.\n",
      "Verifique se a variável de ambiente SCALA_HOME está definida corretamente: C:\\Users\\renat\\AppData\\Local\\Coursier\\data\n",
      "Versão do Java: \"1.6.0_38\"\n",
      "Erro ao obter a versão do Delta Lake. Verifique se a instalação está correta.\n",
      "Certifique-se de que você instalou o Delta Lake usando 'pip install delta-spark' ou método equivalente, e que sua versão do Spark é compatível com a versão do Delta Lake.\n",
      "Se você estiver usando o Spark com JARs, verifique se a versão do JAR do Delta Lake corresponde à versão do pacote 'delta-spark' instalado.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pyspark.sql import SparkSession\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "def obter_versoes_notebook():\n",
    "    \"\"\"\n",
    "    Obtém e imprime as versões do Python, Spark, Scala, Java e Delta Lake, projetado para execução em um ambiente de notebook Jupyter.\n",
    "    \"\"\"\n",
    "    print(\"Obtendo informações de versão:\")\n",
    "\n",
    "    # 1. Versão do Python\n",
    "    versao_python = sys.version\n",
    "    print(f\"Versão do Python: {versao_python}\")\n",
    "\n",
    "    # 2. Versão do Spark\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"VersaoSpark\").getOrCreate()\n",
    "        versao_spark = spark.version\n",
    "        print(f\"Versão do Spark: {versao_spark}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao obter a versão do Spark: {e}\")\n",
    "        print(f\"Detalhes do erro: {e}\")\n",
    "        versao_spark = None\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "\n",
    "    # 3. Versão do Scala\n",
    "    try:\n",
    "        versao_scala = subprocess.run(['scala', '-version'], capture_output=True, text=True, check=True).stderr\n",
    "        versao_scala = versao_scala.split(\"version \")[1].split(\" \")[0]  # Extract Scala version\n",
    "        print(f\"Versão do Scala: {versao_scala}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Erro ao obter a versão do Scala: {e}\")\n",
    "        print(f\"Detalhes do erro: {e}\")\n",
    "        versao_scala = None\n",
    "    except FileNotFoundError:\n",
    "        print(\"Scala não encontrado. Certifique-se de que o Scala está no seu PATH e que a variável de ambiente SCALA_HOME está definida corretamente.\")\n",
    "        if 'SCALA_HOME' in os.environ:\n",
    "            print(f\"Verifique se a variável de ambiente SCALA_HOME está definida corretamente: {os.environ['SCALA_HOME']}\")\n",
    "        else:\n",
    "            print(\"A variável de ambiente SCALA_HOME não está definida. Defina-a para o diretório de instalação do Scala, por exemplo: C:\\\\Program Files\\\\Scala\")\n",
    "        versao_scala = \"Scala não instalado\"  # Atualizado para refletir a não instalação\n",
    "\n",
    "    # 4. Versão do Java\n",
    "    try:\n",
    "        versao_java = subprocess.run(['java', '-version'], capture_output=True, text=True, check=True).stderr\n",
    "        versao_java = versao_java.split(\"version \")[1].split(\"\\n\")[0].strip()  # Extract Java version\n",
    "        print(f\"Versão do Java: {versao_java}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Erro ao obter a versão do Java: {e}\")\n",
    "        print(f\"Detalhes do erro: {e}\")\n",
    "        versao_java = \"Erro ao executar 'java -version'\" # Atualizado para refletir o erro na execução\n",
    "    except FileNotFoundError:\n",
    "        print(\"Java não encontrado. Certifique-se de que o Java está no seu PATH e que a variável de ambiente JAVA_HOME está definida corretamente.\")\n",
    "        if 'JAVA_HOME' in os.environ:\n",
    "            print(f\"Verifique se a variável de ambiente JAVA_HOME está definida corretamente: {os.environ['JAVA_HOME']}\")\n",
    "        else:\n",
    "            print(\"A variável de ambiente JAVA_HOME não está definida. Defina-a para o diretório de instalação do Java, por exemplo: C:\\\\Program Files\\\\Java\\\\jdk-11.0.10\")\n",
    "        versao_java = \"Java não instalado\" # Atualizado para refletir a não instalação\n",
    "\n",
    "    # 5. Versão do Delta Lake\n",
    "    try:\n",
    "        delta_spec = importlib.util.find_spec(\"delta\")\n",
    "        if delta_spec is not None:\n",
    "            import delta\n",
    "            versao_delta = delta.__version__\n",
    "            print(f\"Versão do Delta Lake: {versao_delta}\")\n",
    "        else:\n",
    "            versao_delta = None\n",
    "            print(\"Delta Lake não está instalado neste ambiente.\")\n",
    "            print(\"Instale-o usando 'pip install delta-spark' ou adicione o JAR do Delta Lake ao Spark.\")\n",
    "    except ImportError as e:\n",
    "        versao_delta = None\n",
    "        print(\"Delta Lake não está instalado neste ambiente. Certifique-se de que o pacote 'delta' está instalado corretamente.\")\n",
    "        print(\"Instale-o usando 'pip install delta-spark' ou verifique sua configuração do Spark, incluindo o seguinte:\")\n",
    "        print(f\"  - Verifique se o pacote 'delta-spark' está instalado no ambiente Python correto (o mesmo que seu kernel do Jupyter Notebook).\")\n",
    "        print(f\"  - Se você estiver usando o Spark com JARs, verifique se o JAR do Delta Lake está incluído na configuração do Spark (por exemplo, em 'spark.jars' ou '--jars').\")\n",
    "        print(f\"Detalhes do erro: {e}\")\n",
    "    except AttributeError:\n",
    "        versao_delta = None\n",
    "        print(\"Erro ao obter a versão do Delta Lake. Verifique se a instalação está correta.\")\n",
    "        print(\"Certifique-se de que você instalou o Delta Lake usando 'pip install delta-spark' ou método equivalente, e que sua versão do Spark é compatível com a versão do Delta Lake.\")\n",
    "        print(\"Se você estiver usando o Spark com JARs, verifique se a versão do JAR do Delta Lake corresponde à versão do pacote 'delta-spark' instalado.\")\n",
    "\n",
    "    return {\n",
    "        \"versao_python\": versao_python,\n",
    "        \"versao_spark\": versao_spark,\n",
    "        \"versao_scala\": versao_scala,\n",
    "        \"versao_java\": versao_java,\n",
    "        \"versao_delta\": versao_delta,\n",
    "    }\n",
    "\n",
    "# Chamando a função e armazenando os resultados\n",
    "versoes = obter_versoes_notebook()\n",
    "\n",
    "# Se você quiser usar as versões em outras células, você pode usar o dicionário 'versoes'\n",
    "# Por exemplo:\n",
    "# if versoes['versao_spark']:\n",
    "#   print(f\"Versão do Spark para uso posterior: {versoes['versao_spark']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH do Python: c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp;C:\\Program Files\\Java\\jdk-17.0.2\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\PuTTY\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Users\\renat\\AppData\\Local\\Android\\Sdk\\platform-tools;C:\\msys64\\mingw64\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Program Files\\Portugal Identity Card\\;C:\\Program Files\\Tesseract-OCR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.3.1\\;C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\renat\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\renat\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\renat\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin;C:\\Users\\renat\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\hadoop\\bin;;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp;C:\\Program Files\\Java\\jdk-17.0.2\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\PuTTY\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Users\\renat\\AppData\\Local\\Android\\Sdk\\platform-tools;C:\\msys64\\mingw64\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Program Files\\Portugal Identity Card\\;C:\\Program Files\\Tesseract-OCR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.3.1\\;C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\renat\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\renat\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\renat\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin;C:\\Users\\renat\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\hadoop\\bin;\n",
      "O PATH do Scala não está configurado corretamente para o Python.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_env = os.environ['PATH']\n",
    "print(f\"PATH do Python: {path_env}\")\n",
    "\n",
    "if \"scala\" in path_env.lower():\n",
    "    print(\"O PATH do Scala parece estar configurado.\")\n",
    "else:\n",
    "    print(\"O PATH do Scala não está configurado corretamente para o Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala não encontrado.\n",
      "Versão do Scala: Scala não instalado\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.environ['SCALA_HOME'] = r'C:\\Users\\renat\\AppData\\Local\\Coursier\\data' # Substitua pelo caminho correto\n",
    "os.environ['PATH'] = os.environ['PATH'] + os.pathsep + os.path.join(os.environ['SCALA_HOME'], 'bin')\n",
    "\n",
    "try:\n",
    "    versao_scala = subprocess.run(['scala', '-version'], capture_output=True, text=True, check=True).stderr\n",
    "    versao_scala = versao_scala.split(\"version \")[1].split(\" \")[0]\n",
    "    print(f\"Versão do Scala: {versao_scala}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Erro ao obter a versão do Scala: {e}\")\n",
    "    print(f\"Detalhes do erro: {e}\")\n",
    "    versao_scala = None\n",
    "except FileNotFoundError:\n",
    "    print(\"Scala não encontrado.\")\n",
    "    versao_scala = \"Scala não instalado\" # Output atualizado para refletir o resultado obtido\n",
    "\n",
    "print(f\"Versão do Scala: {versao_scala}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída: Scala code runner version: 1.5.4\n",
      "Scala version (default): 3.6.4\n",
      "\n",
      "Verificando o PATH do Python:\n",
      "c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp;C:\\Program Files\\Java\\jdk-17.0.2\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\PuTTY\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Users\\renat\\AppData\\Local\\Android\\Sdk\\platform-tools;C:\\msys64\\mingw64\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Program Files\\Portugal Identity Card\\;C:\\Program Files\\Tesseract-OCR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.3.1\\;C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\renat\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\renat\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\renat\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin;C:\\Users\\renat\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\hadoop\\bin;;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp;C:\\Program Files\\Java\\jdk-17.0.2\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\PuTTY\\;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311;C:\\Users\\renat\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Users\\renat\\AppData\\Local\\Android\\Sdk\\platform-tools;C:\\msys64\\mingw64\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Program Files\\Portugal Identity Card\\;C:\\Program Files\\Tesseract-OCR;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.3.1\\;C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\;C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\;C:\\Users\\renat\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\renat\\AppData\\Local\\GitHubDesktop\\bin;C:\\Users\\renat\\AppData\\Local\\Google\\Cloud SDK\\google-cloud-sdk\\bin;C:\\Users\\renat\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\hadoop\\bin;;C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin\n",
      "O diretório do Scala (C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin) está no PATH do Python.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "scala_bat_path = r\"C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin\\scala.bat\" # Substitua pelo caminho correto\n",
    "\n",
    "try:\n",
    "    resultado = subprocess.run([scala_bat_path, '-version'], capture_output=True, text=True, check=True)\n",
    "    print(f\"Saída: {resultado.stdout}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Erro ao executar scala.bat: {e}\")\n",
    "    print(f\"Saída do erro: {e.stderr}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Arquivo scala.bat não encontrado em: {scala_bat_path}\")\n",
    "\n",
    "# Verificar o PATH que o Python está usando\n",
    "print(\"Verificando o PATH do Python:\")\n",
    "print(os.environ['PATH'])\n",
    "\n",
    "# Verificar se o diretório do Scala está no PATH\n",
    "scala_dir = os.path.dirname(scala_bat_path)\n",
    "if scala_dir in os.environ['PATH']:\n",
    "    print(f\"O diretório do Scala ({scala_dir}) está no PATH do Python.\")\n",
    "else:\n",
    "    print(f\"O diretório do Scala ({scala_dir}) não está no PATH do Python.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída completa do scala.bat -version: Scala code runner version: 1.5.4\n",
      "Scala version (default): 3.6.4\n",
      "Versão do Scala: :\n",
      "Versão do Scala: :\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "scala_bat_path = r\"C:\\Users\\renat\\AppData\\Local\\Coursier\\data\\bin\\scala.bat\"  # Substitua pelo caminho correto\n",
    "\n",
    "try:\n",
    "    resultado = subprocess.run([scala_bat_path, '-version'], capture_output=True, text=True, check=True)\n",
    "    versao_scala = resultado.stdout.strip()\n",
    "    print(f\"Saída completa do scala.bat -version: {versao_scala}\")\n",
    "\n",
    "    if \"version\" in versao_scala.lower():\n",
    "        versao_scala = versao_scala.lower().split(\"version\")[1].split()[0]\n",
    "    elif \":\" in versao_scala:\n",
    "        versao_scala = versao_scala.split(\":\")[1].split()[0]\n",
    "    elif \"scala version\" in versao_scala.lower():\n",
    "        versao_scala = versao_scala.lower().split(\"scala version\")[1].split()[0]\n",
    "    else:\n",
    "        print(\"Formato de versão desconhecido. Usando a saída bruta.\")\n",
    "        versao_scala = versao_scala  # Se não encontrar, usa a saída completa.\n",
    "\n",
    "    print(f\"Versão do Scala: {versao_scala}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Erro ao executar scala.bat: {e}\")\n",
    "    print(f\"Saída do erro: {e.stderr}\")\n",
    "    versao_scala = None\n",
    "except FileNotFoundError:\n",
    "    print(f\"Arquivo scala.bat não encontrado em: {scala_bat_path}\")\n",
    "    versao_scala = \"Scala não instalado\"\n",
    "\n",
    "print(f\"Versão do Scala: {versao_scala}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv: An error occurred while calling o78.saveAsTable.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:419)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:406)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createTable(V2SessionCatalog.scala:145)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createTable(V2SessionCatalog.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.CreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:579)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "All 1 files processed (attempted) and loaded into 'bronze.tvde_earnings_history'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\renat\\AppData\\Local\\Temp\\ipykernel_19180\\898348527.py\", line 8, in <module>\n",
      "    .saveAsTable(f'{layer}.{table}')\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py\", line 1586, in saveAsTable\n",
      "    self._jwrite.saveAsTable(name)\n",
      "  File \"c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o78.saveAsTable.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:419)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:406)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createTable(V2SessionCatalog.scala:145)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createTable(V2SessionCatalog.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.CreateTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:86)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:579)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sdf in spark_dataframes_list:\n",
    "    try:\n",
    "        # Write the normalized Spark DataFrame to a Delta table\n",
    "        sdf.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(f'{layer}.{table}')\n",
    "        print(f\"Successfully processed and loaded {filepath} into {layer}.{table}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"All {len(filepath_list)} files processed (attempted) and loaded into '{layer}.{table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
