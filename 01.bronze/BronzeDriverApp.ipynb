{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and/or add data to tvde_earnings_history table in the bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import col, lit, regexp_extract, when, to_date, unix_timestamp, from_unixtime, concat_ws, lpad, create_map\n",
    "#from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Spark warehouse to: file:///C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n",
      "Changed current working directory to: C:\\Users\\renat\\Documents\\imgPdados-finance-uber\\toll-reconciliation-tool\\spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "# layer and table\n",
    "layer = \"bronze\"\n",
    "table = \"tvde_earnings_history\"\n",
    "catalog_name = \"toll_reconciliation_tool\"\n",
    "\n",
    "# Define the desired warehouse location explicitly\n",
    "project_dir = \"C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/spark-warehouse\"\n",
    "warehouse_location = \"file:///\" + os.path.abspath(project_dir)\n",
    "print(f\"Setting Spark warehouse to: {warehouse_location}\")\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(project_dir)\n",
    "print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Functions (select_files, add_files_to_list, normalize_dataframe) ---\n",
    "def select_files():\n",
    "    \"\"\"Opens a file selection dialog and returns a list with the paths of the selected files.\"\"\"\n",
    "    root = tk.Tk()  # Create a Tkinter window\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select files\",  # Window title\n",
    "    )\n",
    "\n",
    "    return list(file_paths)  # Convert the returned tuple to a list\n",
    "\n",
    "def add_files_to_list(file_list):\n",
    "    \"\"\"Adds the paths of the selected files to the list.\"\"\"\n",
    "    selected_file_paths = select_files()\n",
    "\n",
    "    if selected_file_paths:  # Check if the user selected any files\n",
    "        file_list.extend(selected_file_paths)  # Add the paths to the list\n",
    "        print(\"Files added:\")\n",
    "        for file_path in selected_file_paths:\n",
    "            print(file_path)\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "def normalize_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a DataFrame according to specified rules.\"\"\"\n",
    "\n",
    "    df['Date'] = None\n",
    "    df['Earnings'] = None\n",
    "    df['Toll'] = None\n",
    "    df['ServiceFee'] = None\n",
    "    df['Tip'] = None\n",
    "    df['StartTime'] = None\n",
    "    df['TotalTime'] = None\n",
    "    df['Distance'] = None\n",
    "    df['DataInput'] = None\n",
    "\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE)\n",
    "\n",
    "    if year_match and month_match:\n",
    "        year = year_match.group(0)\n",
    "        month = month_match.group(0).lower()\n",
    "        month_number = {\n",
    "            'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "            'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "        }[month]\n",
    "    else:\n",
    "        year = None\n",
    "        month_number = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Extracted Text']\n",
    "\n",
    "        # include control columns\n",
    "        df.at[index, 'DataInput'] = datetime.now()\n",
    "\n",
    "        # Date\n",
    "        date_match = re.search(r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? \\d{2} de [a-z]{3}', text, re.IGNORECASE)\n",
    "        if date_match:\n",
    "            day_month = re.search(r'\\d{2} de [a-z]{3}', date_match.group(0), re.IGNORECASE).group(0)\n",
    "            day = re.search(r'\\d{2}', day_month).group(0)\n",
    "            if year and month_number:\n",
    "                df.at[index, 'Date'] = f\"{year}/{month_number}/{day}\"\n",
    "            else:\n",
    "                df.at[index, 'Date'] = f\"{day}\" \n",
    "        else:\n",
    "            df.at[index, 'Date'] = f'{year}/{month_number}/{day}'\n",
    "\n",
    "\n",
    "        # Earnings\n",
    "        earnings_match = re.search(r'€\\s*([\\d,.]+)', text)\n",
    "        if earnings_match:\n",
    "            df.at[index, 'Earnings'] = earnings_match.group(1).replace(',', '.')\n",
    "\n",
    "        # Toll\n",
    "        for word in toll_words:\n",
    "            toll_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if toll_match:\n",
    "                df.at[index, 'Toll'] = toll_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Service Fee\n",
    "        for word in service_fee_words:\n",
    "            service_match = re.search(r'€\\s*([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if service_match:\n",
    "                df.at[index, 'ServiceFee'] = service_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Tip\n",
    "        for word in tip_words:\n",
    "            tip_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if tip_match:\n",
    "                df.at[index, 'Tip'] = tip_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Start Time\n",
    "        start_time_match = re.search(r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})', text)\n",
    "        if start_time_match:\n",
    "            df.at[index, 'StartTime'] = start_time_match.group(1).replace('.', ':').replace('*', ':').replace('-', ':')\n",
    "\n",
    "        # Total Time\n",
    "        total_time_match = re.search(r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', text)\n",
    "        if total_time_match:\n",
    "            df.at[index, 'TotalTime'] = f\"{total_time_match.group(1)}:{total_time_match.group(2)}\"\n",
    "\n",
    "        # Distance\n",
    "        distance_match = re.search(r'([\\d,.]+)\\s*km', text)\n",
    "        if distance_match:\n",
    "            df.at[index, 'Distance'] = distance_match.group(1).replace(',', '.')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files added:\n",
      "C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n"
     ]
    }
   ],
   "source": [
    "# Selecting Files:\n",
    "filepath_list = [] # Create an empty list\n",
    "add_files_to_list(filepath_list) # Call the function to add files and print the final list\n",
    "pandas_dataframes_list = []  # List to store DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:/Users/renat/Documents/imgPdados-finance-uber/toll-reconciliation-tool/2023-dezembro.csv\n",
      "Number of rows in (252, 1)\n",
      "\n",
      "First few rows of the 1th DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Extracted Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4c81caa9-1297-488e-b0ae-25df10512546",
       "rows": [
        [
         "0",
         "sex , 29 de dez"
        ],
        [
         "1",
         "€ 2,60 1l-46 Taxa de serviço"
        ],
        [
         "2",
         "€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google"
        ],
        [
         "3",
         "€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT"
        ],
        [
         "4",
         "€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sex , 29 de dez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€ 2,60 1l-46 Taxa de serviço</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>€3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€5,22 UberX Saver . 18 min 46 segundos 20-12 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>€4,37 UberX Saver ' 18 min 1l segundos 19.45 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Extracted Text\n",
       "0                                    sex , 29 de dez\n",
       "1                       € 2,60 1l-46 Taxa de serviço\n",
       "2  €3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...\n",
       "3  €5,22 UberX Saver . 18 min 46 segundos 20-12 8...\n",
       "4  €4,37 UberX Saver ' 18 min 1l segundos 19.45 6..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# puts each df in the 'dataframes_list', one for each CSV file.\n",
    "for filepath in filepath_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')  # Use utf-8 encoding\n",
    "        pandas_dataframes_list.append(df)\n",
    "        print(f\"Successfully read {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filepath}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: Empty CSV file - {filepath}\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Parsing error in {filepath}. Check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filepath}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# You can access each DataFrame like this:\n",
    "if pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,df in enumerate(pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {df.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(df.head())  # Print the first few rows of the first DataFrame\n",
    "else:\n",
    "    print(\"\\nNo CSV files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in (252, 10)\n",
      "\n",
      "First few rows of the 1th DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Extracted Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Earnings",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Toll",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ServiceFee",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Tip",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "StartTime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TotalTime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Distance",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DataInput",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5e50b275-81cf-4865-9571-903a2270a59a",
       "rows": [
        [
         "0",
         "sex , 29 de dez",
         "2023/12/29",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "2025-04-20 22:04:44.724336"
        ],
        [
         "1",
         "€ 2,60 1l-46 Taxa de serviço",
         "2023/12/29",
         "2.60",
         null,
         null,
         null,
         null,
         null,
         null,
         "2025-04-20 22:04:44.727330"
        ],
        [
         "2",
         "€3,23 21-23 UberX 6 min 22 segundos 138 km 4 € 0,50 Valor extra Boavista que Oriental da Parque dade do Porto Jardim da Porto Fundação A43 Map data 02025 InstGeogr: Nacional Rua dos Caldeireiros; 4000 Porto; PT Tv; de Alferes Malheiro; 4000-060 Porto; PT Agradecer valor extra Google",
         "2023/12/29",
         "3.23",
         null,
         null,
         "0.50",
         "21:23",
         "6:22",
         "138",
         "2025-04-20 22:04:44.728324"
        ],
        [
         "3",
         "€5,22 UberX Saver . 18 min 46 segundos 20-12 8.22 km Matosinhos N13 Rio Tinto Porto Az0 Foz do Dor Google_ Map data 0202 GooaleInstGcogr Nacional 4050-456 Porto; PT R Fernando Lopes Graça, 4100 431 Porto; PT",
         "2023/12/29",
         "5.22",
         null,
         null,
         null,
         "20:12",
         "18:46",
         "8.22",
         "2025-04-20 22:04:44.728324"
        ],
        [
         "4",
         "€4,37 UberX Saver ' 18 min 1l segundos 19.45 6.89 km Fozdo Douro Porto Ponte Luís Google Lap data 02025 InstGeogr Nacional Cais do Cavaco; 4400 Vila Nova de Gaia, PT Praça de Gomes Teixeira; 4050-161 Porto; PT",
         "2023/12/29",
         "4.37",
         null,
         null,
         null,
         "19:45",
         null,
         "6.89",
         "2025-04-20 22:04:44.728324"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Toll</th>\n",
       "      <th>ServiceFee</th>\n",
       "      <th>Tip</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>TotalTime</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DataInput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sex , 29 de dez</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-04-20 22:04:44.724336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>€ 2,60 1l-46 Taxa de serviço</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>2.60</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-04-20 22:04:44.727330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>€3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>3.23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.50</td>\n",
       "      <td>21:23</td>\n",
       "      <td>6:22</td>\n",
       "      <td>138</td>\n",
       "      <td>2025-04-20 22:04:44.728324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>€5,22 UberX Saver . 18 min 46 segundos 20-12 8...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>5.22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20:12</td>\n",
       "      <td>18:46</td>\n",
       "      <td>8.22</td>\n",
       "      <td>2025-04-20 22:04:44.728324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>€4,37 UberX Saver ' 18 min 1l segundos 19.45 6...</td>\n",
       "      <td>2023/12/29</td>\n",
       "      <td>4.37</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19:45</td>\n",
       "      <td>None</td>\n",
       "      <td>6.89</td>\n",
       "      <td>2025-04-20 22:04:44.728324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Extracted Text        Date Earnings  Toll ServiceFee   Tip StartTime TotalTime Distance                   DataInput\n",
       "0                                    sex , 29 de dez  2023/12/29     None  None       None  None      None      None     None  2025-04-20 22:04:44.724336\n",
       "1                       € 2,60 1l-46 Taxa de serviço  2023/12/29     2.60  None       None  None      None      None     None  2025-04-20 22:04:44.727330\n",
       "2  €3,23 21-23 UberX 6 min 22 segundos 138 km 4 €...  2023/12/29     3.23  None       None  0.50     21:23      6:22      138  2025-04-20 22:04:44.728324\n",
       "3  €5,22 UberX Saver . 18 min 46 segundos 20-12 8...  2023/12/29     5.22  None       None  None     20:12     18:46     8.22  2025-04-20 22:04:44.728324\n",
       "4  €4,37 UberX Saver ' 18 min 1l segundos 19.45 6...  2023/12/29     4.37  None       None  None     19:45      None     6.89  2025-04-20 22:04:44.728324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# normalizing Pandas DataFrame \n",
    "normalized_pandas_dataframes_list = [normalize_dataframe(df.copy(), os.path.basename(filepath)) for df, filepath in zip(pandas_dataframes_list, filepath_list)]\n",
    "\n",
    "\n",
    "# Now 'normalized_dataframes' contains the normalized DataFrames.\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)  # Increase terminal width (adjust as needed)\n",
    "\n",
    "# To view the first normalized DataFrame rows:\n",
    "if normalized_pandas_dataframes_list:  # Check if the list is not empty\n",
    "    for i,ndf in enumerate(normalized_pandas_dataframes_list):\n",
    "        print(f\"Number of rows in {ndf.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(ndf.head())  # Print the first few rows of each DataFrame\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully!\n",
      "Application name: toll_reconciliation_tool\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with the specified warehouse directory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(catalog_name) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Confirming that the session is working\n",
    "if spark:\n",
    "    print(\"Spark session started successfully!\")\n",
    "    print(f\"Application name: {spark.sparkContext.appName}\")\n",
    "else:\n",
    "    print(\"Failed to start Spark session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "|  default|\n",
      "|     gold|\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "The 'bronze' schema exists.\n"
     ]
    }
   ],
   "source": [
    "# Attempt a basic Hive operation\n",
    "try:\n",
    "    # Show a list of layers/schemas/databases in the spark-warehouse\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check if the 'bronze' schema exists\n",
    "result = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "schemas = [row[0] for row in result]\n",
    "\n",
    "if \"bronze\" not in schemas:\n",
    "    print(\"Error: The 'bronze' schema does not exist. Please create it.\")  # error for not finding the bronze layer\n",
    "else:\n",
    "    print(\"The 'bronze' schema exists.\")\n",
    "    # Switch to the bronze layer\n",
    "    spark.sql(f\"USE {layer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert each Pandas DataFrame in the list to a Spark DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark_dataframes_list \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mndf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnormalized_pandas_dataframes_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# To view the first normalized Spark DataFrame rows:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spark_dataframes_list:  \u001b[38;5;66;03m# Check if the list is not empty\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert each Pandas DataFrame in the list to a Spark DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark_dataframes_list \u001b[38;5;241m=\u001b[39m [\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ndf \u001b[38;5;129;01min\u001b[39;00m normalized_pandas_dataframes_list]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# To view the first normalized Spark DataFrame rows:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spark_dataframes_list:  \u001b[38;5;66;03m# Check if the list is not empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:969\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[0;32m    956\u001b[0m     _merge_type,\n\u001b[0;32m    957\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m     ),\n\u001b[0;32m    967\u001b[0m )\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    972\u001b[0m     )\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "\u001b[1;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert each Pandas DataFrame in the list to a Spark DataFrame\n",
    "spark_dataframes_list = [spark.createDataFrame(ndf) for ndf in normalized_pandas_dataframes_list]\n",
    "\n",
    "# To view the first normalized Spark DataFrame rows:\n",
    "if spark_dataframes_list:  # Check if the list is not empty\n",
    "    for i,sdf in enumerate(spark_dataframes_list):\n",
    "        print(f\"Number of rows in {sdf.count()}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(sdf.head())  # Print the first few rows of each DataFrame\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sdf in spark_dataframes_list:\n",
    "    try:\n",
    "        # Write the normalized Spark DataFrame to a Delta table\n",
    "        sdf.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(f'{layer}.{table}')\n",
    "        print(f\"Successfully processed and loaded {filepath} into {layer}.{table}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"All {len(filepath_list)} files processed (attempted) and loaded into '{layer}.{table}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
