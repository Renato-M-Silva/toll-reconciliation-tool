{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d122110c-bdf8-4f30-a646-e2d488627a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a8c1f73-d000-4444-93ae-332682301ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def normalize_dataframe(df, filename):\n",
    "    \"\"\"Normalizes a DataFrame according to specified rules.\"\"\"\n",
    "\n",
    "    df['Date'] = None\n",
    "    df['Earnings'] = None\n",
    "    df['Toll'] = None\n",
    "    df['ServiceFee'] = None\n",
    "    df['Tip'] = None\n",
    "    df['StartTime'] = None\n",
    "    df['TotalTime'] = None\n",
    "    df['Distance'] = None\n",
    "    df['DataInput'] = None\n",
    "\n",
    "    toll_words = [\"pedágio\"]\n",
    "    service_fee_words = [\"taxa de serviço\"]\n",
    "    tip_words = [\"valor extra\"]\n",
    "\n",
    "    # Extract year and month from filename\n",
    "    year_match = re.search(r'\\d{4}', filename)\n",
    "    month_match = re.search(r'(jan|fev|mar|abr|mai|jun|jul|ago|set|out|nov|dez)', filename, re.IGNORECASE)\n",
    "\n",
    "    if year_match and month_match:\n",
    "        year = year_match.group(0)\n",
    "        month = month_match.group(0).lower()\n",
    "        month_number = {\n",
    "            'jan': '01', 'fev': '02', 'mar': '03', 'abr': '04', 'mai': '05', 'jun': '06',\n",
    "            'jul': '07', 'ago': '08', 'set': '09', 'out': '10', 'nov': '11', 'dez': '12'\n",
    "        }[month]\n",
    "    else:\n",
    "        year = None\n",
    "        month_number = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Extracted Text']\n",
    "\n",
    "        # incluir colunas de controle\n",
    "        df.at[index, 'DataInput'] = datetime.now()\n",
    "                    \n",
    "        # Date\n",
    "        date_match = re.search(r'(seg|ter|qua|qui|sex|sáb|dom|Mon|Tue|Wed|Thu|Fri|Sat|Sun)?,? \\d{2} de [a-z]{3}', text, re.IGNORECASE)\n",
    "        if date_match:\n",
    "            day_month = re.search(r'\\d{2} de [a-z]{3}', date_match.group(0), re.IGNORECASE).group(0)\n",
    "            day = re.search(r'\\d{2}', day_month).group(0)\n",
    "            if year and month_number:\n",
    "                df.at[index, 'Date'] = f\"{year}/{month_number}/{day}\"\n",
    "            else:\n",
    "                df.at[index, 'Date'] = f\"{day}\" \n",
    "        else:\n",
    "            df.at[index, 'Date'] = f'{year}/{month_number}/{day}'\n",
    "\n",
    "\n",
    "        # Earnings\n",
    "        earnings_match = re.search(r'€\\s*([\\d,.]+)', text)\n",
    "        if earnings_match:\n",
    "            df.at[index, 'Earnings'] = earnings_match.group(1).replace(',', '.')\n",
    "\n",
    "        # Toll\n",
    "        for word in toll_words:\n",
    "            toll_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if toll_match:\n",
    "                df.at[index, 'Toll'] = toll_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Service Fee\n",
    "        for word in service_fee_words:\n",
    "            service_match = re.search(r'€\\s*([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if service_match:\n",
    "                df.at[index, 'ServiceFee'] = service_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Tip\n",
    "        for word in tip_words:\n",
    "            tip_match = re.search(r'([\\d,.]+)\\s*' + word, text, re.IGNORECASE)\n",
    "            if tip_match:\n",
    "                df.at[index, 'Tip'] = tip_match.group(1).replace(',', '.')\n",
    "                break\n",
    "\n",
    "        # Start Time\n",
    "        start_time_match = re.search(r'(\\d{2}-\\d{2}|\\d{2}\\.\\d{2}|\\d{2}\\*\\d{2})', text)\n",
    "        if start_time_match:\n",
    "            df.at[index, 'StartTime'] = start_time_match.group(1).replace('.', ':').replace('*', ':').replace('-', ':')\n",
    "\n",
    "        # Total Time\n",
    "        total_time_match = re.search(r'(\\d+)\\s*min\\s*(\\d+)\\s*seg', text)\n",
    "        if total_time_match:\n",
    "            df.at[index, 'TotalTime'] = f\"{total_time_match.group(1)}:{total_time_match.group(2)}\"\n",
    "\n",
    "        # Distance\n",
    "        distance_match = re.search(r'([\\d,.]+)\\s*km', text)\n",
    "        if distance_match:\n",
    "            df.at[index, 'Distance'] = distance_match.group(1).replace(',', '.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e137c3-3a3a-48b6-a59a-ae35617452a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Nome do database\n",
    "database = \"bronze\"\n",
    "tabela = \"tvde_earnings_history\"\n",
    "\n",
    "#path\n",
    "file_path_list = ['https://raw.githubusercontent.com/Renato-M-Silva/toll-reconciliation-tool/refs/heads/main/2023-dezembro.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f38c917-8d9f-4aa3-87f6-856792205502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selecting  Files:\n",
    "dataframe_list = []  # List to store DataFrames\n",
    "\n",
    "for filepath in file_path_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')  # Use utf-8 encoding\n",
    "        dataframe_list.append(df)\n",
    "        print(f\"Successfully read {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {filepath}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: Empty CSV file - {filepath}\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Parsing error in {filepath}. Check the file format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading {filepath}: {e}\")\n",
    "\n",
    "# Now 'dataframes' contains a list of DataFrames, one for each CSV file.\n",
    "\n",
    "# You can access each DataFrame like this:\n",
    "if dataframe_list:  # Check if the list is not empty\n",
    "    for i,df in enumerate(dataframe_list):\n",
    "        print(f\"Number of rows in {df.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(df.head())  # Print the first few rows of the first DataFrame\n",
    "else:\n",
    "    print(\"\\nNo CSV files were successfully read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b565a6-beba-4344-99f1-bdbc6e707fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# normalizing\n",
    "normalized_dataframes_list = [normalize_dataframe(df.copy(), os.path.basename(filepath)) for df, filepath in zip(dataframe_list, file_path_list)]\n",
    "\n",
    "# Now 'normalized_dataframes' contains the normalized DataFrames.\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 700)  # Increase terminal width (adjust as needed)\n",
    "\n",
    "# To view the first normalized DataFrame rows:\n",
    "if normalized_dataframes_list:  # Check if the list is not empty\n",
    "    for i,df in enumerate(normalized_dataframes_list):\n",
    "        print(f\"Number of rows in {df.shape}\")\n",
    "        print(f\"\\nFirst few rows of the {i+1}th DataFrame:\")\n",
    "        display(df.head())  # Print the first few rows of the first DataFrame\n",
    "else:\n",
    "    print(\"\\nNo files were successfully read.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac361a1-7d4c-4d73-87fb-d55035937156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#From Pandas to Spark and then to SQL\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Pandas to Spark\").getOrCreate()\n",
    "\n",
    "# Convert each Pandas DataFrame in the list to a Spark DataFrame\n",
    "spark_dataframes_list = [spark.createDataFrame(df) for df in normalized_dataframes_list]\n",
    "\n",
    "# Saves data in Delta format\n",
    "for df in spark_dataframes_list:\n",
    "    df.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('overwrite') \\\n",
    "        .option('mergeSchema', 'true') \\\n",
    "        .option('overwriteSchema', 'true') \\\n",
    "        .saveAsTable(f'{database}.{tabela}')\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f573847-0711-4877-a35b-8d88495224f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC DETAIL bronze.tvde_earnings_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7d68c6-7fc0-46d2-b608-a9a52a4fccad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select *\n",
    "from bronze.tvde_earnings_history"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6267998926990129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BronzeDriverApp",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
